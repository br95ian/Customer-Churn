# -*- coding: utf-8 -*-
"""ChurningCustomers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rPHXzX0rXfh_HQ8iWT3HL5bn-As3Za35
"""

pip install tensorflow

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, Dropout, LeakyReLU
from tensorflow.keras.utils import to_categorical

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/AI Projects/CustomerChurn_dataset (1).csv')

df

print(df.isnull().sum())

df.dtypes

for col in df:
    if df[col].dtype == 'O':  # Check if the column type is object (string)
        df[col] = pd.factorize(df[col])[0]

df.dtypes

df.astype(float)

from sklearn.ensemble import RandomForestClassifier

X = df.drop('Churn', axis=1)
y = df['Churn']

"""Feature Engineering"""

model = RandomForestClassifier()

model.fit(X, y)

feature_importances = model.feature_importances_



feature_names = X.columns
feature_imp =pd.DataFrame({'feature': feature_names, 'importance': feature_importances})

best=feature_imp.sort_values(by='importance', ascending=False)

best.head(10)

plt.figure(figsize=(10, 6))
plt.barh(best['feature'], best['importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()

df=df[['tenure','MonthlyCharges','customerID','TotalCharges','Contract','PaymentMethod', 'OnlineSecurity','TechSupport','gender','OnlineBackup','InternetService',
       'PaperlessBilling','MultipleLines','Partner','StreamingMovies' ]]

df.info()



X=df

import keras
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler, MinMaxScaler

sc=StandardScaler()
X_scaled = sc.fit_transform(X)

# new DataFrame with the scaled features
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

data = pd.concat([X_scaled_df, y], axis=1)

data

from sklearn.metrics import roc_auc_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

"""MLP"""

input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer1_dropout = Dropout(0.5)(hidden_layer_1)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer2_dropout = Dropout(0.5)(hidden_layer_2)
hidden_layer_3 = Dense(14, activation='relu')(hidden_layer_2)
hidden_layer3_dropout = Dropout(0.5)(hidden_layer_3)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=34, batch_size=32, validation_data=(X_test, y_test))

_, accuracy = model.evaluate(X_train, y_train)
accuracy*100

_, val_accuracy = model.evaluate(X_val, y_val)
val_accuracy*100

from sklearn.model_selection import KFold,GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from scikeras.wrappers import KerasClassifier

def create_model(activation='relu', dropout_rate=0.5):
    input_layer = Input(shape=(X_train.shape[1],))
    hidden_layer_1 = Dense(32, activation='relu')(input_layer)
    hidden_layer1_dropout = Dropout(dropout_rate)(hidden_layer_1)

    hidden_layer_2 = Dense(24, activation=activation)(hidden_layer1_dropout)
    hidden_layer2_dropout = Dropout(dropout_rate)(hidden_layer_2)

    hidden_layer_3 = Dense(14, activation=activation)(hidden_layer2_dropout)
    hidden_layer3_dropout = Dropout(dropout_rate)(hidden_layer_3)

    output_layer = Dense(1, activation='sigmoid')(hidden_layer3_dropout)
    model = Model(inputs=input_layer, outputs=output_layer)

    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    return model


param_grid = {
    'epochs': [20,25,28,32,34,50, 100],
    'dropout_rate': [0.5],
    'batch_size': [32, 64, 128]
}


mlp = KerasClassifier(build_fn=create_model,dropout_rate=0.5)

# Use GridSearchCV to perform grid search
grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=5, scoring='accuracy',n_jobs=-1)
grid_search.fit(X_train, y_train)

bestp=grid_search.best_params_
# Print the best parameters found
print("Best Parameters:", bestp)

# Evaluate the best model on the test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)

model.save("/Users/brian/Desktop/Python/bestmodel.h5")

